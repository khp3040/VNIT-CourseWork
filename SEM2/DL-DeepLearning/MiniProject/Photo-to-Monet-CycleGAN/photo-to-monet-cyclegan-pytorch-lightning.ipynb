{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07067f2b",
   "metadata": {
    "papermill": {
     "duration": 0.010002,
     "end_time": "2023-04-10T12:17:50.814961",
     "exception": false,
     "start_time": "2023-04-10T12:17:50.804959",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Photo to Monet — CycleGAN⚡PyTorch Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351ae0cb",
   "metadata": {
    "papermill": {
     "duration": 0.008369,
     "end_time": "2023-04-10T12:17:50.832220",
     "exception": false,
     "start_time": "2023-04-10T12:17:50.823851",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook aims to implement CycleGAN. The model architecture is adapted from the [tutorial](https://www.kaggle.com/code/amyjang/monet-cyclegan-tutorial/notebook) available. We also attempt to convert the code to PyTorch Lightning here. The list of references is as follows:\n",
    "* Original [paper](https://arxiv.org/abs/1703.10593) and [code](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix) for CycleGAN.\n",
    "* Original [paper](https://arxiv.org/abs/1611.04076) for LSGAN, which has shown to outperform BCE loss and is used in the original CycleGAN implementation. BCE loss may be susceptible to vanishing gradient problems and cause ineffective learning. For better training stability, we use LSGAN, which adopts the mean squared error for the adversarial criterion.\n",
    "* [Documentation](https://pytorch-lightning.readthedocs.io/en/stable/notebooks/lightning_examples/basic-gan.html) for basic GAN in PyTorch Lightning. Kaggle seems to still be using [Python 3.7](https://www.kaggle.com/discussions/product-feedback/388376) at the time of writing, which does not support Lightning 2.0. Manual optimization is required for training with multiple optimizers in Lightning 2.0, but for simplicity we stick to automatic optimization here in version 1.9. To upgrade the code for compatibility with Lightning 2.0, a useful documentation can be found [here](https://lightning.ai/docs/pytorch/stable/upgrade/from_1_9.html).\n",
    "\n",
    "More work can be done to include evaluation metrics like the inception score (IS) or Fréchet inception distance (FID)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "410506e0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-04-10T12:17:50.853121Z",
     "iopub.status.busy": "2023-04-10T12:17:50.852345Z",
     "iopub.status.idle": "2023-04-10T12:17:58.179414Z",
     "shell.execute_reply": "2023-04-10T12:17:58.177552Z"
    },
    "papermill": {
     "duration": 7.341446,
     "end_time": "2023-04-10T12:17:58.182244",
     "exception": false,
     "start_time": "2023-04-10T12:17:50.840798",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 127] The specified procedure could not be found. Error loading \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\lib\\c10_cuda.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mL\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pytorch_lightning\\__init__.py:34\u001b[0m\n\u001b[0;32m     31\u001b[0m     _logger\u001b[38;5;241m.\u001b[39maddHandler(logging\u001b[38;5;241m.\u001b[39mStreamHandler())\n\u001b[0;32m     32\u001b[0m     _logger\u001b[38;5;241m.\u001b[39mpropagate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning_fabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m seed_everything  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callback  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LightningDataModule, LightningModule  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\lightning_fabric\\__init__.py:23\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# In PyTorch 2.0+, setting this variable will force `torch.cuda.is_available()` and `torch.cuda.device_count()`\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# to use an NVML-based implementation that doesn't poison forks.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/83973\u001b[39;00m\n\u001b[0;32m     20\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTORCH_NVML_BASED_CUDA_CHECK\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning_fabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Fabric  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning_fabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m seed_everything  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     26\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFabric\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed_everything\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\lightning_fabric\\fabric.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, cast, Dict, Generator, List, Mapping, Optional, overload, Sequence, Tuple, Union\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning_utilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply_func\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apply_to_collection\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\__init__.py:129\u001b[0m\n\u001b[0;32m    127\u001b[0m     err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(last_error)\n\u001b[0;32m    128\u001b[0m     err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    131\u001b[0m     is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 127] The specified procedure could not be found. Error loading \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\lib\\c10_cuda.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pytorch_lightning as L\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from pytorch_lightning.trainer.supporters import CombinedLoader\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.io import read_image\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "L.seed_everything(0, workers=True)\n",
    "print(L.__name__, L.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb4f860",
   "metadata": {
    "papermill": {
     "duration": 0.008854,
     "end_time": "2023-04-10T12:17:58.200575",
     "exception": false,
     "start_time": "2023-04-10T12:17:58.191721",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287fad7e",
   "metadata": {
    "papermill": {
     "duration": 0.008739,
     "end_time": "2023-04-10T12:17:58.218174",
     "exception": false,
     "start_time": "2023-04-10T12:17:58.209435",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b0e517",
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2023-04-10T12:17:58.237604Z",
     "iopub.status.busy": "2023-04-10T12:17:58.237291Z",
     "iopub.status.idle": "2023-04-10T12:17:58.243434Z",
     "shell.execute_reply": "2023-04-10T12:17:58.242343Z"
    },
    "papermill": {
     "duration": 0.018821,
     "end_time": "2023-04-10T12:17:58.246013",
     "exception": false,
     "start_time": "2023-04-10T12:17:58.227192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_img(img_tensor, nrow, title=\"\"):\n",
    "    img_tensor = img_tensor.detach().cpu()*0.5 + 0.5\n",
    "    img_grid = make_grid(img_tensor, nrow=nrow).permute(1, 2, 0)\n",
    "    plt.figure(figsize=(18, 8))\n",
    "    plt.imshow(img_grid)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e490edf0",
   "metadata": {
    "papermill": {
     "duration": 0.008661,
     "end_time": "2023-04-10T12:17:58.263303",
     "exception": false,
     "start_time": "2023-04-10T12:17:58.254642",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Augmenting the images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d57222",
   "metadata": {
    "papermill": {
     "duration": 0.008538,
     "end_time": "2023-04-10T12:17:58.280896",
     "exception": false,
     "start_time": "2023-04-10T12:17:58.272358",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Before loading the datasets, we define `CustomTransform` for image augmentation. This improves learning by introducing more variety in the images during training instead of learning from the same set of images, especially when we only have 300 Monet paintings. We look at some basic image transformations:\n",
    "* Scaling the images larger using `Resize` and then randomly cropping to the original size of 256 with `RandomCrop` to obtain slightly different images.\n",
    "* Randomly flipping the images horizontally using `RandomHorizontalFlip`. The photos and Monet paintings do not greatly depend on the horizontal orientation.\n",
    "* Randomly changing the colors of the images using `ColorJitter`. This could mimic different lighting conditions for the photos and introduce variability in the colors of the Monet paintings.\n",
    "\n",
    "Other possible transformations can be found [here](https://pytorch.org/vision/stable/transforms.html). These transformations are only needed during model training/fitting, and we specify this using the `stage` argument. Finally, the images are scaled down for better convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afae0f9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-10T12:17:58.300317Z",
     "iopub.status.busy": "2023-04-10T12:17:58.299733Z",
     "iopub.status.idle": "2023-04-10T12:17:58.307468Z",
     "shell.execute_reply": "2023-04-10T12:17:58.306451Z"
    },
    "papermill": {
     "duration": 0.02013,
     "end_time": "2023-04-10T12:17:58.309638",
     "exception": false,
     "start_time": "2023-04-10T12:17:58.289508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomTransform(object):\n",
    "    def __init__(self, load_dim=286, target_dim=256):\n",
    "        self.transform_train = T.Compose([\n",
    "            T.Resize((load_dim, load_dim)),\n",
    "            T.RandomCrop((target_dim, target_dim)),\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.ColorJitter(brightness=0.2, contrast=0.2,\n",
    "                          saturation=0.2, hue=0.1),\n",
    "        ])\n",
    "        \n",
    "        # ensure images outside of training dataset are also of the same size\n",
    "        self.transform = T.Resize((target_dim, target_dim))\n",
    "        \n",
    "    def __call__(self, img, stage=\"fit\"):\n",
    "        if stage == \"fit\":\n",
    "            img = self.transform_train(img)\n",
    "        else:\n",
    "            img = self.transform(img)\n",
    "        return img*2 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce099c77",
   "metadata": {
    "papermill": {
     "duration": 0.008638,
     "end_time": "2023-04-10T12:17:58.327204",
     "exception": false,
     "start_time": "2023-04-10T12:17:58.318566",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Storing the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53443059",
   "metadata": {
    "papermill": {
     "duration": 0.008599,
     "end_time": "2023-04-10T12:17:58.344687",
     "exception": false,
     "start_time": "2023-04-10T12:17:58.336088",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To load and store the datasets, we define a custom `Dataset` involving three main methods: \n",
    "* `__init__` to initialize the dataset.\n",
    "* `__len__` to retrieve the size of the dataset.\n",
    "* `__getitem__` to get the i-th sample of images after performing the transformations described above.\n",
    "\n",
    "Similarly, we define the `stage` argument to differentiate between the training dataset and prediction dataset when performing the transformations. Different instances of `CustomDataset` will be used to retrieve the photos and Monet paintings separately. We look at combining them while iterating through the datasets later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0632f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-10T12:17:58.363383Z",
     "iopub.status.busy": "2023-04-10T12:17:58.363114Z",
     "iopub.status.idle": "2023-04-10T12:17:58.368846Z",
     "shell.execute_reply": "2023-04-10T12:17:58.367793Z"
    },
    "papermill": {
     "duration": 0.01765,
     "end_time": "2023-04-10T12:17:58.371051",
     "exception": false,
     "start_time": "2023-04-10T12:17:58.353401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, filenames, transform, stage):\n",
    "        self.filenames = filenames\n",
    "        self.transform = transform\n",
    "        self.stage = stage\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.filenames[idx]\n",
    "        img = read_image(img_name) / 255.0\n",
    "        return self.transform(img, stage=self.stage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edbaf1c",
   "metadata": {
    "papermill": {
     "duration": 0.00872,
     "end_time": "2023-04-10T12:17:58.388768",
     "exception": false,
     "start_time": "2023-04-10T12:17:58.380048",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Iterating through the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74b6b7c",
   "metadata": {
    "papermill": {
     "duration": 0.008602,
     "end_time": "2023-04-10T12:17:58.406181",
     "exception": false,
     "start_time": "2023-04-10T12:17:58.397579",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To prepare the datasets, we load them into `DataLoader` separately, which can then iterate through the datasets as needed. Because the training dataset contains both the Monet paintings and photos, we pass both dataloaders into `CombinedLoader` for training. We specify the sampling mode using `mode=\"max_size_cycle\"` to stop after one complete pass of the larger dataset of photos while cycling through the smaller dataset of Monet paintings. Other modes can be found [here](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.utilities.combined_loader.html). In contrast, we fix the prediction dataset to only contain photos for simplicity since we are trying to generate Monet-style images in this notebook.\n",
    "\n",
    "To organize all the steps described above for processing data, we define a custom `LightningDataModule`. A datamodule involves many methods, but we are mainly concerned with:\n",
    "* `setup` to create the datasets and apply the corresponding transformations defined above.\n",
    "* `train_dataloader` to generate the dataloader for the training dataset.\n",
    "* `predict_dataloader` to generate the dataloader for the prediction dataset.\n",
    "\n",
    "Other possible methods can be found [here](https://pytorch-lightning.readthedocs.io/en/stable/data/datamodule.html). We define the following parameters:\n",
    "* `DEBUG` — to enable debugging mode in CPU.\n",
    "* `MONET_DIR` and `PHOTO_DIR` — the directories where the Monet paintings and photos are loaded from.\n",
    "* `BATCH_SIZE` — the number of samples in each training/prediction batch.\n",
    "* `num_workers` — the number of subprocesses (excluding the main process) used for data loading.\n",
    "* `pin_memory` — to enable faster data transfer to GPU during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cef88a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-10T12:17:58.425380Z",
     "iopub.status.busy": "2023-04-10T12:17:58.424584Z",
     "iopub.status.idle": "2023-04-10T12:17:58.534455Z",
     "shell.execute_reply": "2023-04-10T12:17:58.533430Z"
    },
    "papermill": {
     "duration": 0.122142,
     "end_time": "2023-04-10T12:17:58.537118",
     "exception": false,
     "start_time": "2023-04-10T12:17:58.414976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MONET_DIR = \"monet_jpg/*.jpg\"\n",
    "PHOTO_DIR = \"photo_jpg/*.jpg\"\n",
    "DEBUG = not torch.cuda.is_available()\n",
    "BATCH_SIZE = [1, 4] # [batch size for Monet paintings, batch size for photos]\n",
    "LOADER_CONFIG = {\n",
    "    \"num_workers\": os.cpu_count(),\n",
    "    \"pin_memory\": torch.cuda.is_available(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab124f23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-10T12:17:58.556624Z",
     "iopub.status.busy": "2023-04-10T12:17:58.556312Z",
     "iopub.status.idle": "2023-04-10T12:17:58.568731Z",
     "shell.execute_reply": "2023-04-10T12:17:58.567818Z"
    },
    "papermill": {
     "duration": 0.024647,
     "end_time": "2023-04-10T12:17:58.570939",
     "exception": false,
     "start_time": "2023-04-10T12:17:58.546292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        debug=DEBUG,\n",
    "        monet_dir=MONET_DIR,\n",
    "        photo_dir=PHOTO_DIR,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        loader_config=LOADER_CONFIG,\n",
    "        transform=CustomTransform(),\n",
    "        mode=\"max_size_cycle\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if isinstance(batch_size, list):\n",
    "            self.batch_size = batch_size  \n",
    "        else:\n",
    "            self.batch_size = [batch_size] * 2\n",
    "        if debug:\n",
    "            idx = max(self.batch_size) * 2\n",
    "        else:\n",
    "            idx = None\n",
    "        self.monet_filenames = sorted(glob.glob(monet_dir))[:idx]\n",
    "        self.photo_filenames = sorted(glob.glob(photo_dir))[:idx]\n",
    "        self.loader_config = loader_config\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        \n",
    "    def setup(self, stage):\n",
    "        if stage == \"fit\":\n",
    "            self.train_monet = CustomDataset(self.monet_filenames, self.transform, stage)\n",
    "            self.train_photo = CustomDataset(self.photo_filenames, self.transform, stage)\n",
    "        \n",
    "        elif stage == \"predict\":\n",
    "            self.predict = CustomDataset(self.photo_filenames, self.transform, stage)\n",
    "            \n",
    "    def train_dataloader(self):\n",
    "        loader_monet = DataLoader(\n",
    "            self.train_monet,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            batch_size=self.batch_size[0],\n",
    "            **self.loader_config,\n",
    "        )\n",
    "        loader_photo = DataLoader(\n",
    "            self.train_photo,\n",
    "            shuffle=True, \n",
    "            drop_last=True,\n",
    "            batch_size=self.batch_size[1],\n",
    "            **self.loader_config,\n",
    "        )\n",
    "        loaders = {\"monet\": loader_monet, \"photo\": loader_photo}\n",
    "        return CombinedLoader(loaders, mode=self.mode)\n",
    "    \n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.predict,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            batch_size=self.batch_size[1],\n",
    "            **self.loader_config,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d95698f",
   "metadata": {
    "papermill": {
     "duration": 0.009015,
     "end_time": "2023-04-10T12:17:58.588899",
     "exception": false,
     "start_time": "2023-04-10T12:17:58.579884",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We check that the datamodule defined is working as intended by visualizing samples of the images below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bb8bb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-10T12:17:58.608788Z",
     "iopub.status.busy": "2023-04-10T12:17:58.608468Z",
     "iopub.status.idle": "2023-04-10T12:18:04.385550Z",
     "shell.execute_reply": "2023-04-10T12:18:04.384407Z"
    },
    "papermill": {
     "duration": 5.797345,
     "end_time": "2023-04-10T12:18:04.395500",
     "exception": false,
     "start_time": "2023-04-10T12:17:58.598155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 5\n",
    "dm_sample = CustomDataModule(batch_size=SAMPLE_SIZE)\n",
    "\n",
    "dm_sample.setup(\"fit\")\n",
    "train_loader = dm_sample.train_dataloader()\n",
    "monet_samples, photo_samples = next(iter(train_loader)).values()\n",
    "\n",
    "dm_sample.setup(\"predict\")\n",
    "predict_loader = dm_sample.predict_dataloader()\n",
    "photo__samples = next(iter(predict_loader)) # used to track performance of model during training later\n",
    "\n",
    "show_img(monet_samples, nrow=SAMPLE_SIZE, title=\"Augmented Monet Paintings\")\n",
    "show_img(photo_samples, nrow=SAMPLE_SIZE, title=\"Augmented Photos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be05eb15",
   "metadata": {
    "papermill": {
     "duration": 0.033364,
     "end_time": "2023-04-10T12:18:04.465238",
     "exception": false,
     "start_time": "2023-04-10T12:18:04.431874",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f214429",
   "metadata": {
    "papermill": {
     "duration": 0.029234,
     "end_time": "2023-04-10T12:18:04.523485",
     "exception": false,
     "start_time": "2023-04-10T12:18:04.494251",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Building CycleGAN Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a97514",
   "metadata": {
    "papermill": {
     "duration": 0.029471,
     "end_time": "2023-04-10T12:18:04.582330",
     "exception": false,
     "start_time": "2023-04-10T12:18:04.552859",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Generator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c8537e",
   "metadata": {
    "papermill": {
     "duration": 0.028951,
     "end_time": "2023-04-10T12:18:04.640190",
     "exception": false,
     "start_time": "2023-04-10T12:18:04.611239",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://lh5.googleusercontent.com/9kNO6hxYJmpcfG5bOjnDazieeLC7Q8jZJi3gTtnJelbkOUL7Xz9e-3F_SNuxPpo4fZ4=w2400\" width=\"600\"/>\n",
    "\n",
    "_Example of the U-Net architecture [[source](https://paperswithcode.com/method/u-net)]._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e17d9d9",
   "metadata": {
    "papermill": {
     "duration": 0.032874,
     "end_time": "2023-04-10T12:18:04.701911",
     "exception": false,
     "start_time": "2023-04-10T12:18:04.669037",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We use a U-Net architecture for the CycleGAN generator. U-Net is a network which consists of downsampling blocks and upsampling blocks with long skip connections, giving it the U-shaped architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ca142f",
   "metadata": {
    "papermill": {
     "duration": 0.029319,
     "end_time": "2023-04-10T12:18:04.760488",
     "exception": false,
     "start_time": "2023-04-10T12:18:04.731169",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Downsampling blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c06cbdc",
   "metadata": {
    "papermill": {
     "duration": 0.028535,
     "end_time": "2023-04-10T12:18:04.817947",
     "exception": false,
     "start_time": "2023-04-10T12:18:04.789412",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The downsampling blocks use convolution layers to increase the number of feature maps while reducing the dimensions of the 2D image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7966caa2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-10T12:18:04.878730Z",
     "iopub.status.busy": "2023-04-10T12:18:04.878347Z",
     "iopub.status.idle": "2023-04-10T12:18:04.886609Z",
     "shell.execute_reply": "2023-04-10T12:18:04.885678Z"
    },
    "papermill": {
     "duration": 0.041424,
     "end_time": "2023-04-10T12:18:04.888705",
     "exception": false,
     "start_time": "2023-04-10T12:18:04.847281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Downsampling(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size=4,\n",
    "        stride=2,\n",
    "        padding=1,\n",
    "        norm=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n",
    "                      stride=stride, padding=padding, bias=not norm),\n",
    "        )\n",
    "        if norm:\n",
    "            self.block.append(nn.InstanceNorm2d(out_channels, affine=True))\n",
    "        self.block.append(nn.LeakyReLU(0.3))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6c8393",
   "metadata": {
    "papermill": {
     "duration": 0.028961,
     "end_time": "2023-04-10T12:18:04.946579",
     "exception": false,
     "start_time": "2023-04-10T12:18:04.917618",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Upsampling blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80da763a",
   "metadata": {
    "papermill": {
     "duration": 0.029235,
     "end_time": "2023-04-10T12:18:05.004855",
     "exception": false,
     "start_time": "2023-04-10T12:18:04.975620",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "On the other hand, the upsampling blocks contain transposed convolution layers, which combine the learned features to output an image with the original size 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b351bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-10T12:18:05.066275Z",
     "iopub.status.busy": "2023-04-10T12:18:05.065368Z",
     "iopub.status.idle": "2023-04-10T12:18:05.072468Z",
     "shell.execute_reply": "2023-04-10T12:18:05.071539Z"
    },
    "papermill": {
     "duration": 0.04013,
     "end_time": "2023-04-10T12:18:05.074530",
     "exception": false,
     "start_time": "2023-04-10T12:18:05.034400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Upsampling(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size=4,\n",
    "        stride=2,\n",
    "        padding=1,\n",
    "        output_padding=0,\n",
    "        dropout=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, \n",
    "                               padding=padding, output_padding=output_padding, bias=False),\n",
    "            nn.InstanceNorm2d(out_channels, affine=True),\n",
    "        )\n",
    "        if dropout:\n",
    "            self.block.append(nn.Dropout(0.5))\n",
    "        self.block.append(nn.ReLU())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029ef420",
   "metadata": {
    "papermill": {
     "duration": 0.029612,
     "end_time": "2023-04-10T12:18:05.133222",
     "exception": false,
     "start_time": "2023-04-10T12:18:05.103610",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Building the generator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fad4ee",
   "metadata": {
    "papermill": {
     "duration": 0.030037,
     "end_time": "2023-04-10T12:18:05.193191",
     "exception": false,
     "start_time": "2023-04-10T12:18:05.163154",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "With the building blocks defined, we can now build our CycleGAN generator. In the upsampling path, we concatenate the outputs of the upsampling blocks and the outputs of the downsampling blocks symmetrically. This can be seen as a kind of skip connection, facilitating information flow in deep networks and reducing the impact of vanishing gradients. For reference, the output size of each block is commented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c14d48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-10T12:18:05.254325Z",
     "iopub.status.busy": "2023-04-10T12:18:05.253624Z",
     "iopub.status.idle": "2023-04-10T12:18:05.264908Z",
     "shell.execute_reply": "2023-04-10T12:18:05.263678Z"
    },
    "papermill": {
     "duration": 0.044618,
     "end_time": "2023-04-10T12:18:05.266964",
     "exception": false,
     "start_time": "2023-04-10T12:18:05.222346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hid_channels):\n",
    "        super().__init__()\n",
    "        self.downsampling_path = nn.Sequential(\n",
    "            Downsampling(in_channels, hid_channels, norm=False), # 64x128x128\n",
    "            Downsampling(hid_channels, hid_channels*2), # 128x64x64\n",
    "            Downsampling(hid_channels*2, hid_channels*4), # 256x32x32\n",
    "            Downsampling(hid_channels*4, hid_channels*8), # 512x16x16\n",
    "            Downsampling(hid_channels*8, hid_channels*8), # 512x8x8\n",
    "            Downsampling(hid_channels*8, hid_channels*8), # 512x4x4\n",
    "            Downsampling(hid_channels*8, hid_channels*8), # 512x2x2\n",
    "            Downsampling(hid_channels*8, hid_channels*8, norm=False), # 512x1x1, instance norm does not work on 1x1\n",
    "        )\n",
    "        self.upsampling_path = nn.Sequential(\n",
    "            Upsampling(hid_channels*8, hid_channels*8, dropout=True), # (512+512)x2x2\n",
    "            Upsampling(hid_channels*16, hid_channels*8, dropout=True), # (512+512)x4x4\n",
    "            Upsampling(hid_channels*16, hid_channels*8, dropout=True), # (512+512)x8x8\n",
    "            Upsampling(hid_channels*16, hid_channels*8), # (512+512)x16x16\n",
    "            Upsampling(hid_channels*16, hid_channels*4), # (256+256)x32x32\n",
    "            Upsampling(hid_channels*8, hid_channels*2), # (128+128)x64x64\n",
    "            Upsampling(hid_channels*4, hid_channels), # (64+64)x128x128\n",
    "        )\n",
    "        self.feature_block = nn.Sequential(\n",
    "            nn.ConvTranspose2d(hid_channels*2, out_channels,\n",
    "                               kernel_size=4, stride=2, padding=1), # 3x256x256\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        for down in self.downsampling_path:\n",
    "            x = down(x)\n",
    "            skips.append(x)\n",
    "        skips = reversed(skips[:-1])\n",
    "\n",
    "        for up, skip in zip(self.upsampling_path, skips):\n",
    "            x = up(x)\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "        return self.feature_block(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f79622",
   "metadata": {
    "papermill": {
     "duration": 0.028879,
     "end_time": "2023-04-10T12:18:05.324416",
     "exception": false,
     "start_time": "2023-04-10T12:18:05.295537",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ae822e",
   "metadata": {
    "papermill": {
     "duration": 0.028723,
     "end_time": "2023-04-10T12:18:05.382585",
     "exception": false,
     "start_time": "2023-04-10T12:18:05.353862",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://lh6.googleusercontent.com/UhJiaTOQWgfHQlWq50IMGBvdkJ3NDggC449cxud8XVlSxUrule8f5LyoLUV8aaYemGw=w2400\" width=\"300\"/>\n",
    "\n",
    "_Diagram of how the PatchGAN discriminator works [[source](https://www.researchgate.net/figure/PatchGAN-discriminator-Each-value-of-the-output-matrix-represents-the-probability-of_fig1_323904616)]._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd12080",
   "metadata": {
    "papermill": {
     "duration": 0.029298,
     "end_time": "2023-04-10T12:18:05.441047",
     "exception": false,
     "start_time": "2023-04-10T12:18:05.411749",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Unlike conventional networks that output a single probability of the input image being real or fake, CycleGAN uses the PatchGAN discriminator that outputs a matrix of values. Intuitively, each value of the output matrix checks the corresponding portion of the input image. Values closer to 1 indicate real classification and values closer to 0 indicate fake classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b776816",
   "metadata": {
    "papermill": {
     "duration": 0.028739,
     "end_time": "2023-04-10T12:18:05.498591",
     "exception": false,
     "start_time": "2023-04-10T12:18:05.469852",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Building the discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f727bbe5",
   "metadata": {
    "papermill": {
     "duration": 0.030542,
     "end_time": "2023-04-10T12:18:05.558200",
     "exception": false,
     "start_time": "2023-04-10T12:18:05.527658",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In general, the PatchGAN discriminator consists of a sequence of convolution layers, which can be built using the downsampling blocks defined earlier. For reference, the output size of each block is commented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c4da22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-10T12:18:05.627566Z",
     "iopub.status.busy": "2023-04-10T12:18:05.626896Z",
     "iopub.status.idle": "2023-04-10T12:18:05.633489Z",
     "shell.execute_reply": "2023-04-10T12:18:05.632439Z"
    },
    "papermill": {
     "duration": 0.043121,
     "end_time": "2023-04-10T12:18:05.635575",
     "exception": false,
     "start_time": "2023-04-10T12:18:05.592454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels, hid_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            Downsampling(in_channels, hid_channels, norm=False), # 64x128x128\n",
    "            Downsampling(hid_channels, hid_channels*2), # 128x64x64\n",
    "            Downsampling(hid_channels*2, hid_channels*4), # 256x32x32\n",
    "            Downsampling(hid_channels*4, hid_channels*8, stride=1), # 512x31x31\n",
    "            nn.Conv2d(hid_channels*8, 1, kernel_size=4, padding=1), # 1x30x30\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec84532f",
   "metadata": {
    "papermill": {
     "duration": 0.031516,
     "end_time": "2023-04-10T12:18:05.696853",
     "exception": false,
     "start_time": "2023-04-10T12:18:05.665337",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### CycleGAN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e71b39",
   "metadata": {
    "papermill": {
     "duration": 0.030486,
     "end_time": "2023-04-10T12:18:05.765524",
     "exception": false,
     "start_time": "2023-04-10T12:18:05.735038",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "With the generator and discriminator defined, we can now build CycleGAN, which consists of two generators and two discriminators:\n",
    "* Generator for photo-to-Monet translation (`gen_PM`).\n",
    "* Generator for Monet-to-photo translation (`gen_MP`).\n",
    "* Discriminator for Monet paintings (`disc_M`).\n",
    "* Discriminator for photos (`disc_P`).\n",
    "\n",
    "Using `init_weights` function, the weights of the layers in the generators and discriminators are initialized using the normal distribution and the biases are initialized to 0s. The Adam optimizer is used for model training. To optimize the parameters, we need to define the loss functions:\n",
    "* **Discriminator loss** (`disc_loss`). For real images fed into the discriminator, the output matrix is compared against a matrix of 1s using the mean squared error. For fake images, the output matrix is compared against a matrix of 0s. This suggests that to minimize loss, the perfect discriminator outputs a matrix of 1s for real images and a matrix of 0s for fake images.\n",
    "* **Generator loss** (`gen_loss`). This is composed of three different loss functions below.\n",
    "  * *Adversarial loss* (`adv_loss`). Fake images are fed into the discriminator and the output matrix is compared against a matrix of 1s using the mean squared error. To minimize loss, the generator needs to 'fool' the discriminator into thinking that the fake images are real and output a matrix of 1s.\n",
    "  * *Identity loss* (`id_loss`). When a Monet painting is fed into the photo-to-Monet generator, we should get back the same Monet painting because nothing needs to be transformed. The same applies for photos fed into the Monet-to-photo generator. To encourage identity mapping, the difference in pixel values between the input image and generated image is measured using the l1 loss.\n",
    "  * *Cycle loss* (`cycle_loss`). When a Monet painting is fed into the Monet-to-photo generator, and the generated image is fed back into the photo-to-Monet generator, it should transform back into the original Monet painting. The same applies for photos passed to the two generators to get back the original photos. To preserve information throughout this cycle, the l1 loss is used to measure the difference between the original image and the cycled image.\n",
    "\n",
    "From the above, the mean squared error and the l1 loss are defined as the adversarial criterion (`adv_criterion`) and the reconstruction criterion (`recon_criterion`) respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347a9b8d",
   "metadata": {
    "papermill": {
     "duration": 0.029657,
     "end_time": "2023-04-10T12:18:05.825352",
     "exception": false,
     "start_time": "2023-04-10T12:18:05.795695",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Building the CycleGAN model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee1c5c7",
   "metadata": {
    "papermill": {
     "duration": 0.029458,
     "end_time": "2023-04-10T12:18:05.884037",
     "exception": false,
     "start_time": "2023-04-10T12:18:05.854579",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To organize the code for modeling, we define the above functions within the `LightningModule` class together with the following methods:\n",
    "* `__init__` to initialize the two generators, the two discriminators, and other parameters.\n",
    "* `forward` to generate Monet-style images given input photos.\n",
    "* `configure_optimizers` to initialize the Adam optimizers and learning rate schedules. We use a constant learning rate and then linearly decay towards the end of training.\n",
    "* `training_step` to compute the loss functions for the generators and discriminators.\n",
    "* `training_epoch_end` to print the average values of the loss functions over the batches per epoch, and visualize the performance of `gen_PM`. We also record the values of the learning rates and losses for plotting later.\n",
    "* `predict_step` to run the `forward` method during prediction.\n",
    "\n",
    "Other useful methods can be found [here](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html). Besides the above, we define additional methods like `get_lr_scheduler` to set up the learning rate schedules, `loss_plot` to plot the loss curves, and `lr_plot` to plot the learning rate schedules. The following parameters are set:\n",
    "* `IN_CHANNELS` — the number of input channels for the generator and discriminator, which equals 3 since we are working with RGB images.\n",
    "* `OUT_CHANNELS` — the number of output channels for the generator, which equals 3 as we are trying to output RGB images.\n",
    "* `HID_CHANNELS` — the number of output channels in the first layer for the generator and discriminator.\n",
    "* `LR` and `BETAS` — the learning rate and beta parameters for the Adam optimizer.\n",
    "* `LAMBDA_ID` and `LAMBDA_CYCLE` — the weights used in the identity loss and cycle loss.\n",
    "* `NUM_EPOCHS` — the number of epochs for training.\n",
    "* `DECAY_EPOCHS` — the number of epochs before starting the learning rate decay.\n",
    "* `DISPLAY_EPOCHS` — the frequency to visualize the performance of `gen_PM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb27db2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-10T12:18:05.944397Z",
     "iopub.status.busy": "2023-04-10T12:18:05.944052Z",
     "iopub.status.idle": "2023-04-10T12:18:05.949179Z",
     "shell.execute_reply": "2023-04-10T12:18:05.948074Z"
    },
    "papermill": {
     "duration": 0.038522,
     "end_time": "2023-04-10T12:18:05.951314",
     "exception": false,
     "start_time": "2023-04-10T12:18:05.912792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IN_CHANNELS = 3\n",
    "OUT_CHANNELS = 3\n",
    "HID_CHANNELS = 64\n",
    "LR = 2e-4\n",
    "BETAS = (0.5, 0.999)\n",
    "LAMBDA_ID = 2\n",
    "LAMBDA_CYCLE = 5\n",
    "NUM_EPOCHS = 36 if not DEBUG else 2\n",
    "DECAY_EPOCHS = 27 if not DEBUG else 1\n",
    "DISPLAY_EPOCHS = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bd2660",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-10T12:18:06.012885Z",
     "iopub.status.busy": "2023-04-10T12:18:06.012519Z",
     "iopub.status.idle": "2023-04-10T12:18:06.046569Z",
     "shell.execute_reply": "2023-04-10T12:18:06.045462Z"
    },
    "papermill": {
     "duration": 0.068008,
     "end_time": "2023-04-10T12:18:06.048679",
     "exception": false,
     "start_time": "2023-04-10T12:18:05.980671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CycleGAN(L.LightningModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels=IN_CHANNELS,\n",
    "        out_channels=OUT_CHANNELS, \n",
    "        hid_channels=HID_CHANNELS,\n",
    "        lr=LR,\n",
    "        betas=BETAS,\n",
    "        lambda_id=LAMBDA_ID,\n",
    "        lambda_cycle=LAMBDA_CYCLE,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        decay_epochs=DECAY_EPOCHS,\n",
    "        display_epochs=DISPLAY_EPOCHS,\n",
    "        photo_samples=photo__samples,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.lambda_id = lambda_id\n",
    "        self.lambda_cycle = lambda_cycle\n",
    "        self.num_epochs = num_epochs\n",
    "        self.decay_epochs = decay_epochs\n",
    "        self.display_epochs = display_epochs\n",
    "        self.photo_samples = photo_samples.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # record learning rates and losses\n",
    "        self.lr_history = [self.lr]\n",
    "        self.loss_names = [\"gen_loss_PM\", \"gen_loss_MP\", \"disc_loss_M\", \"disc_loss_P\"]\n",
    "        self.loss_history = {loss: [] for loss in self.loss_names}\n",
    "        \n",
    "        # initialize generators and discriminators\n",
    "        self.gen_PM = Generator(in_channels, out_channels, hid_channels)\n",
    "        self.gen_MP = Generator(in_channels, out_channels, hid_channels)\n",
    "        self.disc_M = Discriminator(in_channels, hid_channels)\n",
    "        self.disc_P = Discriminator(in_channels, hid_channels)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, z):\n",
    "        return self.gen_PM(z)\n",
    "                \n",
    "    def init_weights(self):\n",
    "        def init_fn(m):\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)\n",
    "            elif isinstance(m, nn.InstanceNorm2d):\n",
    "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "                \n",
    "        self.gen_PM = self.gen_PM.apply(init_fn)\n",
    "        self.gen_MP = self.gen_MP.apply(init_fn)\n",
    "        self.disc_M = self.disc_M.apply(init_fn)\n",
    "        self.disc_P = self.disc_P.apply(init_fn)\n",
    "        \n",
    "    def adv_criterion(self, y_hat, y):\n",
    "        return F.mse_loss(y_hat, y)\n",
    "    \n",
    "    def recon_criterion(self, y_hat, y):\n",
    "        return F.l1_loss(y_hat, y)\n",
    "    \n",
    "    def adv_loss(self, fake_Y, disc_Y):\n",
    "        fake_Y_hat = disc_Y(fake_Y)\n",
    "        valid = torch.ones_like(fake_Y_hat)\n",
    "        adv_loss_XY = self.adv_criterion(fake_Y_hat, valid)\n",
    "        return adv_loss_XY\n",
    "    \n",
    "    def id_loss(self, real_Y, gen_XY):\n",
    "        id_Y = gen_XY(real_Y)\n",
    "        id_loss_Y = self.recon_criterion(id_Y, real_Y)\n",
    "        return self.lambda_id * id_loss_Y\n",
    "    \n",
    "    def cycle_loss(self, real_Y, fake_X, gen_XY):\n",
    "        cycle_Y = gen_XY(fake_X)\n",
    "        cycle_loss_Y = self.recon_criterion(cycle_Y, real_Y)\n",
    "        return self.lambda_cycle * cycle_loss_Y\n",
    "    \n",
    "    def gen_loss(self, real_X, real_Y, gen_XY, gen_YX, disc_Y):\n",
    "        fake_Y = gen_XY(real_X)\n",
    "        fake_X = gen_YX(real_Y)\n",
    "        \n",
    "        adv_loss_XY = self.adv_loss(fake_Y, disc_Y)\n",
    "        id_loss_Y = self.id_loss(real_Y, gen_XY)\n",
    "        cycle_loss_Y = self.cycle_loss(real_Y, fake_X, gen_XY)\n",
    "        cycle_loss_X = self.cycle_loss(real_X, fake_Y, gen_YX)\n",
    "        total_cycle_loss = cycle_loss_X + cycle_loss_Y\n",
    "        \n",
    "        gen_loss_XY = adv_loss_XY + id_loss_Y + total_cycle_loss\n",
    "        return gen_loss_XY\n",
    "    \n",
    "    def disc_loss(self, real_Y, fake_Y, disc_Y):\n",
    "        real_Y_hat = disc_Y(real_Y)\n",
    "        valid = torch.ones_like(real_Y_hat)\n",
    "        real_loss_Y = self.adv_criterion(real_Y_hat, valid)\n",
    "        \n",
    "        fake_Y_hat = disc_Y(fake_Y.detach())\n",
    "        fake = torch.zeros_like(fake_Y_hat)\n",
    "        fake_loss_Y = self.adv_criterion(fake_Y_hat, fake)\n",
    "        \n",
    "        disc_loss_Y = (fake_loss_Y+real_loss_Y) * 0.5\n",
    "        return disc_loss_Y\n",
    "    \n",
    "    def get_lr_scheduler(self, optimizer):\n",
    "        def lr_lambda(epoch):\n",
    "            val = 1.0 - max(0, epoch-self.decay_epochs+1.0)/(self.num_epochs-self.decay_epochs+1.0)\n",
    "            return max(0.0, val)\n",
    "        return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        params = {\n",
    "            \"lr\": self.lr,\n",
    "            \"betas\": self.betas,\n",
    "        }\n",
    "        opt_gen_PM = torch.optim.Adam(self.gen_PM.parameters(), **params)\n",
    "        opt_gen_MP = torch.optim.Adam(self.gen_MP.parameters(), **params)\n",
    "        opt_disc_M = torch.optim.Adam(self.disc_M.parameters(), **params)\n",
    "        opt_disc_P = torch.optim.Adam(self.disc_P.parameters(), **params)\n",
    "        optimizers = [opt_gen_PM, opt_gen_MP, opt_disc_M, opt_disc_P]\n",
    "        schedulers = [self.get_lr_scheduler(opt) for opt in optimizers]\n",
    "        return optimizers, schedulers\n",
    "    \n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        real_M = batch[\"monet\"]\n",
    "        real_P = batch[\"photo\"]\n",
    "        if optimizer_idx == 0:\n",
    "            gen_loss_PM = self.gen_loss(real_P, real_M, \n",
    "                                        self.gen_PM, self.gen_MP, self.disc_M)\n",
    "            return gen_loss_PM\n",
    "        \n",
    "        if optimizer_idx == 1:\n",
    "            gen_loss_MP = self.gen_loss(real_M, real_P,\n",
    "                                        self.gen_MP, self.gen_PM, self.disc_P)\n",
    "            return gen_loss_MP\n",
    "        \n",
    "        if optimizer_idx == 2:\n",
    "            with torch.no_grad():\n",
    "                fake_M = self.gen_PM(real_P)\n",
    "            disc_loss_M = self.disc_loss(real_M, fake_M,\n",
    "                                         self.disc_M)\n",
    "            return disc_loss_M\n",
    "        \n",
    "        if optimizer_idx == 3:\n",
    "            with torch.no_grad():\n",
    "                fake_P = self.gen_MP(real_M)\n",
    "            disc_loss_P = self.disc_loss(real_P, fake_P,\n",
    "                                         self.disc_P)\n",
    "            return disc_loss_P\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        current_epoch = self.current_epoch + 1\n",
    "        \n",
    "        lr = self.lr_schedulers()[0].get_last_lr()[0]\n",
    "        self.lr_history.append(lr)\n",
    "        losses = {}\n",
    "        for j in range(4):\n",
    "            loss = [out[j][\"loss\"].item() for out in outputs]\n",
    "            self.loss_history[self.loss_names[j]].extend(loss)\n",
    "            losses[self.loss_names[j]] = np.mean(loss)\n",
    "        print(\n",
    "            \" - \".join([\n",
    "                f\"Epoch {current_epoch}\",\n",
    "                f\"lr: {self.lr_history[-2]:.5f}\",\n",
    "                *[f\"{loss}: {val:.5f}\" for loss, val in losses.items()],\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        if current_epoch%self.display_epochs==0 or current_epoch in [1, self.num_epochs]:\n",
    "            torch.set_grad_enabled(False)\n",
    "            self.eval()\n",
    "            gen_monets = self.forward(self.photo_samples)\n",
    "            show_img(\n",
    "                torch.cat([self.photo_samples, gen_monets]),\n",
    "                nrow=len(self.photo_samples),\n",
    "                title=f\"Epoch {current_epoch}: Photo-to-Monet Translation\",\n",
    "            )\n",
    "            torch.set_grad_enabled(True)\n",
    "            self.train()\n",
    "            \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        return self.forward(batch)\n",
    "    \n",
    "    def lr_plot(self):\n",
    "        num_epochs = len(self.lr_history[:-1])\n",
    "        plt.figure(figsize=(18, 4.5))\n",
    "        plt.title(\"Learning Rate Schedule\")\n",
    "        plt.ylabel(\"Learning Rate\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.plot(\n",
    "            np.arange(1, num_epochs+1),\n",
    "            self.lr_history[:-1],\n",
    "        )\n",
    "            \n",
    "    def loss_plot(self):\n",
    "        titles = [\"Generator Loss Curves\", \"Discriminator Loss Curves\"]\n",
    "        num_steps = len(list(self.loss_history.values())[0])\n",
    "        plt.figure(figsize=(18, 4.5))\n",
    "        for j in range(4):\n",
    "            if j%2 == 0:\n",
    "                plt.subplot(1, 2, (j//2)+1)\n",
    "                plt.title(titles[j//2])\n",
    "                plt.ylabel(\"Loss\")\n",
    "                plt.xlabel(\"Step\")\n",
    "            plt.plot(\n",
    "                np.arange(1, num_steps+1),\n",
    "                self.loss_history[self.loss_names[j]],\n",
    "                label=self.loss_names[j],\n",
    "            )\n",
    "            plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a65a234",
   "metadata": {
    "papermill": {
     "duration": 0.031805,
     "end_time": "2023-04-10T12:18:06.113169",
     "exception": false,
     "start_time": "2023-04-10T12:18:06.081364",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6458b0",
   "metadata": {
    "papermill": {
     "duration": 0.029962,
     "end_time": "2023-04-10T12:18:06.172812",
     "exception": false,
     "start_time": "2023-04-10T12:18:06.142850",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64926a5",
   "metadata": {
    "papermill": {
     "duration": 0.028916,
     "end_time": "2023-04-10T12:18:06.230861",
     "exception": false,
     "start_time": "2023-04-10T12:18:06.201945",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To start training the model, we use `Trainer` to automatically handle the training loop by running the `fit` method. We set the below parameters in `TRAIN_CONFIG` for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcc8594",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-10T12:18:06.292192Z",
     "iopub.status.busy": "2023-04-10T12:18:06.291811Z",
     "iopub.status.idle": "2023-04-10T12:18:06.297128Z",
     "shell.execute_reply": "2023-04-10T12:18:06.296048Z"
    },
    "papermill": {
     "duration": 0.038983,
     "end_time": "2023-04-10T12:18:06.299283",
     "exception": false,
     "start_time": "2023-04-10T12:18:06.260300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_CONFIG = {\n",
    "    \"accelerator\": \"gpu\" if not DEBUG else \"cpu\",\n",
    "    \"devices\": 1,\n",
    "    \"logger\": False,\n",
    "    \"enable_checkpointing\": True,\n",
    "    \"max_epochs\": NUM_EPOCHS,\n",
    "    \"precision\": 16 if not DEBUG else 32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4153dc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-10T12:18:06.359746Z",
     "iopub.status.busy": "2023-04-10T12:18:06.359158Z",
     "iopub.status.idle": "2023-04-10T17:15:06.589987Z",
     "shell.execute_reply": "2023-04-10T17:15:06.588801Z"
    },
    "papermill": {
     "duration": 17820.265189,
     "end_time": "2023-04-10T17:15:06.592851",
     "exception": false,
     "start_time": "2023-04-10T12:18:06.327662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dm = CustomDataModule()\n",
    "model = CycleGAN()\n",
    "trainer = L.Trainer(**TRAIN_CONFIG)\n",
    "\n",
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6239e8a7",
   "metadata": {
    "papermill": {
     "duration": 0.120798,
     "end_time": "2023-04-10T17:15:06.844250",
     "exception": false,
     "start_time": "2023-04-10T17:15:06.723452",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Plotting the learning rate schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8e40e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-10T17:15:07.107166Z",
     "iopub.status.busy": "2023-04-10T17:15:07.105846Z",
     "iopub.status.idle": "2023-04-10T17:15:07.405557Z",
     "shell.execute_reply": "2023-04-10T17:15:07.404477Z"
    },
    "papermill": {
     "duration": 0.439791,
     "end_time": "2023-04-10T17:15:07.407931",
     "exception": false,
     "start_time": "2023-04-10T17:15:06.968140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.lr_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd57341b",
   "metadata": {
    "papermill": {
     "duration": 0.121184,
     "end_time": "2023-04-10T17:15:07.651119",
     "exception": false,
     "start_time": "2023-04-10T17:15:07.529935",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Plotting the loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d03ab7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-10T17:15:07.886601Z",
     "iopub.status.busy": "2023-04-10T17:15:07.885671Z",
     "iopub.status.idle": "2023-04-10T17:15:09.482068Z",
     "shell.execute_reply": "2023-04-10T17:15:09.481038Z"
    },
    "papermill": {
     "duration": 1.717585,
     "end_time": "2023-04-10T17:15:09.484772",
     "exception": false,
     "start_time": "2023-04-10T17:15:07.767187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.loss_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5f7c36",
   "metadata": {
    "papermill": {
     "duration": 0.124385,
     "end_time": "2023-04-10T17:15:09.821944",
     "exception": false,
     "start_time": "2023-04-10T17:15:09.697559",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74d5caa",
   "metadata": {
    "papermill": {
     "duration": 0.118902,
     "end_time": "2023-04-10T17:15:10.061823",
     "exception": false,
     "start_time": "2023-04-10T17:15:09.942921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdb0625",
   "metadata": {
    "papermill": {
     "duration": 0.12115,
     "end_time": "2023-04-10T17:15:10.303534",
     "exception": false,
     "start_time": "2023-04-10T17:15:10.182384",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Computing the predictions can be done by running the `predict` method to generate the Monet-style images given the input photos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ef77d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-10T17:15:10.548863Z",
     "iopub.status.busy": "2023-04-10T17:15:10.546450Z",
     "iopub.status.idle": "2023-04-10T17:15:48.555268Z",
     "shell.execute_reply": "2023-04-10T17:15:48.554093Z"
    },
    "papermill": {
     "duration": 38.132619,
     "end_time": "2023-04-10T17:15:48.557953",
     "exception": false,
     "start_time": "2023-04-10T17:15:10.425334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = trainer.predict(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21e13c9",
   "metadata": {
    "papermill": {
     "duration": 0.117597,
     "end_time": "2023-04-10T17:15:48.794916",
     "exception": false,
     "start_time": "2023-04-10T17:15:48.677319",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Saving the generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46d9654",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-10T17:15:49.036996Z",
     "iopub.status.busy": "2023-04-10T17:15:49.036362Z",
     "iopub.status.idle": "2023-04-10T17:16:23.846964Z",
     "shell.execute_reply": "2023-04-10T17:16:23.845797Z"
    },
    "papermill": {
     "duration": 35.05728,
     "end_time": "2023-04-10T17:16:23.970379",
     "exception": false,
     "start_time": "2023-04-10T17:15:48.913099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"../images\", exist_ok=True)\n",
    "\n",
    "idx = 0\n",
    "for tensor in predictions:\n",
    "    for monet in tensor:\n",
    "        save_image((monet.float().squeeze()*0.5+0.5), fp=f\"../images/{idx}.jpg\")\n",
    "        idx += 1\n",
    "\n",
    "shutil.make_archive(\"/kaggle/working/images\", \"zip\", \"/kaggle/images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4693c12c",
   "metadata": {
    "papermill": {
     "duration": 0.117062,
     "end_time": "2023-04-10T17:16:24.207140",
     "exception": false,
     "start_time": "2023-04-10T17:16:24.090078",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Examining the results on other photo samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2780799",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-10T17:16:24.444484Z",
     "iopub.status.busy": "2023-04-10T17:16:24.443851Z",
     "iopub.status.idle": "2023-04-10T17:16:34.825608Z",
     "shell.execute_reply": "2023-04-10T17:16:34.824327Z"
    },
    "papermill": {
     "duration": 10.504302,
     "end_time": "2023-04-10T17:16:34.828374",
     "exception": false,
     "start_time": "2023-04-10T17:16:24.324072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "model.eval()\n",
    "\n",
    "for j, photos in enumerate(iter(predict_loader)):\n",
    "    if j == 5:\n",
    "        break\n",
    "    gen_monets = model(photos)\n",
    "    show_img(\n",
    "        torch.cat([photos, gen_monets]),\n",
    "        nrow=len(photos),\n",
    "        title=f\"Sample {j+1}: Photo-to-Monet Translation\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7612cf2",
   "metadata": {
    "papermill": {
     "duration": 0.252164,
     "end_time": "2023-04-10T17:16:35.333684",
     "exception": false,
     "start_time": "2023-04-10T17:16:35.081520",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17937.511846,
   "end_time": "2023-04-10T17:16:38.762935",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-10T12:17:41.251089",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "065ddf90fb49491f8a8abbed5ea789c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "1d907f89c4fd4cc8b06c651215d70341": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "20b29c0cf03649ef879666e61331ff59": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "221cb5cec9af49038640ac8c699bd776": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2dba49ac307c40d590b489ea08d05e51": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_221cb5cec9af49038640ac8c699bd776",
       "max": 1759,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_20b29c0cf03649ef879666e61331ff59",
       "value": 1759
      }
     },
     "2e3d48bb96f34c8796b500d55245c482": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "341a09835aed40efbdbfd939a9107adf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4a6068cffb4e469e8081ba263c7c05c2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "5b00e8cef6fe4ea8ae6f53cb16283efe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_341a09835aed40efbdbfd939a9107adf",
       "placeholder": "​",
       "style": "IPY_MODEL_ff4a4e145ca445b9987d302d5bfa5f04",
       "value": " 1760/1760 [00:35&lt;00:00, 49.54it/s]"
      }
     },
     "63cc8d191ed34edb97c6f3a021fbee7a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "64434fa14b1c4fb093f9cc488080ba83": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "794cdca0e5c44861b4b93b4c15736538": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "7b3641533b3f4268bd946e46f83582e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_64434fa14b1c4fb093f9cc488080ba83",
       "placeholder": "​",
       "style": "IPY_MODEL_2e3d48bb96f34c8796b500d55245c482",
       "value": " 1759/1759 [08:15&lt;00:00,  3.55it/s, loss=0.535]"
      }
     },
     "8b6437092a85422485f8c0bc67df9942": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c263f159f5bb422b9c038f889b5ca748",
        "IPY_MODEL_2dba49ac307c40d590b489ea08d05e51",
        "IPY_MODEL_7b3641533b3f4268bd946e46f83582e2"
       ],
       "layout": "IPY_MODEL_f71d9080e4c949efb97d6d88f504c019"
      }
     },
     "8e62b05526eb4cc4a15e64beb61fff9b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e80116e1096c44a8baa4ff73aff1c6fe",
       "placeholder": "​",
       "style": "IPY_MODEL_4a6068cffb4e469e8081ba263c7c05c2",
       "value": "Predicting DataLoader 0: 100%"
      }
     },
     "b15a3d2d0e54440fbb7cc12c3bf7eb0b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8e62b05526eb4cc4a15e64beb61fff9b",
        "IPY_MODEL_c865f0e4db0a40aa8b22e028193b7000",
        "IPY_MODEL_5b00e8cef6fe4ea8ae6f53cb16283efe"
       ],
       "layout": "IPY_MODEL_794cdca0e5c44861b4b93b4c15736538"
      }
     },
     "bc3ecd19347d47dbbb31fb43ed95dda2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c263f159f5bb422b9c038f889b5ca748": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_bc3ecd19347d47dbbb31fb43ed95dda2",
       "placeholder": "​",
       "style": "IPY_MODEL_63cc8d191ed34edb97c6f3a021fbee7a",
       "value": "Epoch 35: 100%"
      }
     },
     "c865f0e4db0a40aa8b22e028193b7000": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1d907f89c4fd4cc8b06c651215d70341",
       "max": 1760,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_065ddf90fb49491f8a8abbed5ea789c1",
       "value": 1760
      }
     },
     "e80116e1096c44a8baa4ff73aff1c6fe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f71d9080e4c949efb97d6d88f504c019": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "ff4a4e145ca445b9987d302d5bfa5f04": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
